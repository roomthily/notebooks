{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun with markov chain generators and metadata (abstracts).\n",
    "\n",
    "\n",
    "good one:\n",
    "This allows the radar site to users in order to more accurately map the position of the tow boat taking into account the length of the divers based on the selected VCP.\n",
    "\n",
    "GIS methods.The California Ocean Uses Atlas Project fills a critical element in the Denver Federal Center, Colorado, for long-term research, education and outreach activity information; media event at the 5 cm depth and predicted climate scenarios, we are modeling the predicted resources will make to mineral supplies in the GIS coverage.Reports concentration of geophysical data in support of nautical chart compilation for safe navigation and to provide background data for engineers, scientific, and other commercial and industrial activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from random import choice\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's build a corpus of many, so many words\n",
    "\n",
    "corpus = ''\n",
    "number_of_abstracts = 0\n",
    "for f in glob.glob('/Users/sscott/Documents/working_bits/solr_superset/parsed/*.json'):\n",
    "#     if number_of_abstracts > 10:\n",
    "#         break\n",
    "    \n",
    "    with open(f, 'r') as g:\n",
    "        data = json.loads(g.read())\n",
    "    \n",
    "    service = data.get('service_description', {})\n",
    "    if not service:\n",
    "        continue\n",
    "    \n",
    "    service = service.get('service', {})\n",
    "    if not service:\n",
    "        continue\n",
    "    \n",
    "    abstract = service.get('abstract', [])\n",
    "    if not abstract or not [a for a in abstract if a]:\n",
    "        continue\n",
    "    \n",
    "    abstract = [a for a in abstract if a]\n",
    "    try:\n",
    "        corpus += ' '.join(abstract)\n",
    "    except:\n",
    "        print abstract\n",
    "    number_of_abstracts += 1\n",
    "    \n",
    "with open('abstract_corpus.txt', 'w') as g:\n",
    "    g.write(corpus.encode('utf-8','ignore'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# onto the basic dict building\n",
    "eos = ['.', '?', '!']\n",
    "splitter = r'.!?,/(){}|; '\n",
    "\n",
    "def _parse(terms):\n",
    "    words = {}\n",
    "    for i, term in enumerate(terms):\n",
    "        try:\n",
    "            first_term, second_term, third_term = terms[i], terms[i+1], terms[i+2]\n",
    "        except IndexError:\n",
    "            break\n",
    "        key = (first_term, second_term)\n",
    "        if key not in words:\n",
    "            words[key] = []\n",
    "        words[key].append(third_term)\n",
    "    return words\n",
    "\n",
    "# and the generator\n",
    "def _generate(words):\n",
    "    li = [key for key in words.keys() if key[0][0].isupper()]\n",
    "    key = choice(li)\n",
    " \n",
    "    li = []\n",
    "    first, second = key\n",
    "    li.append(first)\n",
    "    li.append(second)\n",
    "    while True:\n",
    "        try:\n",
    "            third = choice(words[key])\n",
    "        except KeyError:\n",
    "            break\n",
    "        li.append(third)\n",
    "        if third[-1] in eos:\n",
    "            break\n",
    "        # else\n",
    "        key = (second, third)\n",
    "        first, second = key\n",
    " \n",
    "    return ' '.join(li)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Park dr.,97229, Portland, OR. True\n",
      "\n",
      "Project. In the index to societal impacts. False\n",
      "\n",
      "Lock by Corps personnel and towing vessels with AIS transponders in 100 meter grid cells. False\n",
      "\n",
      "Defense Installation Spatial Data Delivery (SDD) project.This dataset contains the Telephone Exchange territories, Exchange Name, Company Name, Area Code, Prefix(es), Use, OCN(Operating Company Number) and Rate CentInformation concerning status and trends of biological resources, focusing on survey design and methodsThe Rail Network is a water wave or a logical geographic area, magnitude, destructiveness, and other commercial and industrial activities. False\n",
      "\n",
      "CentInformation concerning status and trends of biological resources, focusing on the technical details of the information for Kure Atoll. False\n",
      "\n",
      "CARIS Export option \"BASE Surface to Image\" was then exported from the Census TIGER database. False\n",
      "\n",
      "Channel aboard the R/V Kilo Moana collected in the density of all possible structure numbers from the USGS on mineral resources and ground water basin that may be approved only after it is not defined, the Census Bureau data, and true color imagery and was compiled by F.T. False\n",
      "\n",
      "Mesocyclone Detection Algorithm from NEXRAD (Level-III TVS Product); Preliminary Local Storm Reports from the NWS National Hurricane Center (NHC), updated hourly, as well as the total population and the St. False\n",
      "\n",
      "India, Japan, United Kingdom, specifically in the dataset (as available from the Climate Database Modernization Program (CDMP). False\n",
      "\n",
      "Sea, 24 nautical mile Exclusive Economic Zone and on the UK and other networks and land use classes; these land surface form class, the high seas, these changes and trends of biological resources, focusing on treesThe NCEP Climate Forecast System Reanalysis (CFSR) was initially retrieved from IHS Inc.'s PI/Dwights PLUS Well Data on CD-ROM, and a major earthquake in San Mateo County. False\n",
      "\n",
      "Junction of US Business west of Devils Lake. False\n",
      "\n",
      "File 3) attribution. True\n",
      "\n",
      "INSIDE Idaho). However, proper processing of raw LiDAR data was used to quantify extreme post-fire landscape response NCALM Project. False\n",
      "\n",
      "USGS carries out the National Aeronautics and Space Administration. False\n",
      "\n",
      "Generated PLS layer with IBM mainframe-based land records. True\n",
      "\n",
      "Mapping (IWG-OCM). Products will include: raw point cloud, 1M DEM, and other fish is limited. False\n",
      "\n",
      "Resources.Utilizing various land protection programs and aid in developing a suite of benthic terrain complexity or \"roughness\". False\n",
      "\n",
      "Sanctuary (less than 110 ft). True\n",
      "\n",
      "DNR Ecological Services Lake Mapping Unit.The National Oceanic and Atmospheric Administration (NOAA) has the statutory mandate to collect hydrographic data in support of nautical chart compilation for safe navigation and to provide background data for the 4-lane project of US Business west of the north would be hurricane research, atmospheric chemistry, thunderstorm investigations, and winter weather missions. False\n",
      "\n",
      "NOAA's Coral Reef National Monument, south of the data set also contains a complete digital hydrologic unit codes for legal and statistical boundaries. False\n",
      "\n",
      "NOAA Aircraft Operation Centers aircraft and then addresses the total catch. False\n",
      "\n",
      "The myths attribute this to Pele's attempt to locate even the help documentation are distributed throughout NOAA Fisheries, State Fisheries, management agencies and academic and international waters in real-time. False\n",
      "\n",
      "Federal Clean Air Act, as well as some non-weather objects like birds and bats we study, and the Joint Nature Conservation Committee (JNCC) Broad Habitats, which encompasses the entire wreck site, and actively engage the public domain, 30-meter resolution land cover data set is an acronym for Arus Lintas Indonesia, meaning 'Indonesian throughflow' in Bahasa Indonesia. False\n",
      "\n",
      "LATITUDE: approximate location, latitude, LONGITUDE: approximate location, longitude, GEN_TYPE: type of interest to scientists studying seismogenic processes at subduction zones. False\n",
      "\n",
      "GIS methods.The California Ocean Uses Atlas Project fills a critical element in the Denver Federal Center, Colorado, for long-term research, education and outreach activity information; media event at the 5 cm depth and predicted climate scenarios, we are modeling the predicted resources will make to mineral supplies in the GIS coverage.Reports concentration of geophysical data in support of nautical chart compilation for safe navigation and to provide background data for engineers, scientific, and other commercial and industrial activities. False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's generate some abstracts!\n",
    "\n",
    "terms = [r for r in re.split(splitter, corpus) if r]\n",
    "words = _parse(terms)\n",
    "\n",
    "for i in xrange(25):\n",
    "    sentence = _generate(words)\n",
    "    print sentence, sentence in corpus\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's try some haikus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "haiku = (5, 7, 5)\n",
    "\n",
    "def generate_line(chain, syllable_count, current):\n",
    "    if len(chain[current]) == 1:\n",
    "        out = chain[current][0]\n",
    "    else:\n",
    "        out = chain[current][randint(0, len(chain[current]) -1)]\n",
    "    \n",
    "    current_syllables = 0\n",
    "    \n",
    "\n",
    "# we need a new corpus structure as markov chain\n",
    "terms = [r for r in re.split(splitter, corpus) if r]\n",
    "markov_chain = {}\n",
    "\n",
    "for i in xrange(len(terms)-1):\n",
    "    if terms[i] in markov_chain:\n",
    "        markov_chain[terms[i]].append(terms[i+1])\n",
    "    else:\n",
    "        markov_chain[terms[i]] = [terms[i+1]]\n",
    "        \n",
    "\n",
    "# start the poem\n",
    "poem = [terms[randint(0, len(terms))]]\n",
    "\n",
    "for s in haiku:\n",
    "    line = None\n",
    "    while line is None:\n",
    "        line = generate_line(markov_chain, s, poem[-1].split()[-1])\n",
    "    poem.append(line)\n",
    "\n",
    "    \n",
    "print poem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
