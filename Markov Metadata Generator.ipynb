{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun with markov chain generators and metadata (abstracts).\n",
    "\n",
    "\n",
    "good one:\n",
    "This allows the radar site to users in order to more accurately map the position of the tow boat taking into account the length of the divers based on the selected VCP.\n",
    "\n",
    "GIS methods.The California Ocean Uses Atlas Project fills a critical element in the Denver Federal Center, Colorado, for long-term research, education and outreach activity information; media event at the 5 cm depth and predicted climate scenarios, we are modeling the predicted resources will make to mineral supplies in the GIS coverage.Reports concentration of geophysical data in support of nautical chart compilation for safe navigation and to provide background data for engineers, scientific, and other commercial and industrial activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from random import choice, randint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's build a corpus of many, so many words\n",
    "\n",
    "corpus = ''\n",
    "number_of_abstracts = 0\n",
    "for f in glob.glob('/Users/sscott/Documents/working_bits/solr_superset/parsed/*.json'):\n",
    "#     if number_of_abstracts > 10:\n",
    "#         break\n",
    "    \n",
    "    with open(f, 'r') as g:\n",
    "        data = json.loads(g.read())\n",
    "    \n",
    "    service = data.get('service_description', {})\n",
    "    if not service:\n",
    "        continue\n",
    "    \n",
    "    service = service.get('service', {})\n",
    "    if not service:\n",
    "        continue\n",
    "    \n",
    "    abstract = service.get('abstract', [])\n",
    "    if not abstract or not [a for a in abstract if a]:\n",
    "        continue\n",
    "    \n",
    "    abstract = [a for a in abstract if a]\n",
    "    try:\n",
    "        corpus += ' '.join(abstract)\n",
    "    except:\n",
    "        print abstract\n",
    "    number_of_abstracts += 1\n",
    "    \n",
    "with open('abstract_corpus.txt', 'w') as g:\n",
    "    g.write(corpus.encode('utf-8','ignore'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# onto the basic dict building\n",
    "eos = ['.', '?', '!']\n",
    "splitter = r'.!?;,/(){}| '\n",
    "\n",
    "def _parse(terms):\n",
    "    words = {}\n",
    "    for i, term in enumerate(terms):\n",
    "        try:\n",
    "            first_term, second_term, third_term = terms[i], terms[i+1], terms[i+2]\n",
    "        except IndexError:\n",
    "            break\n",
    "        key = (first_term, second_term)\n",
    "        if key not in words:\n",
    "            words[key] = []\n",
    "        words[key].append(third_term)\n",
    "    return words\n",
    "\n",
    "# and the generator\n",
    "def _generate(words):\n",
    "    li = [key for key in words.keys() if key[0][0].isupper()]\n",
    "    key = choice(li)\n",
    " \n",
    "    li = []\n",
    "    first, second = key\n",
    "    li.append(first)\n",
    "    li.append(second)\n",
    "    while True:\n",
    "        try:\n",
    "            third = choice(words[key])\n",
    "        except KeyError:\n",
    "            break\n",
    "        li.append(third)\n",
    "        if third[-1] in eos:\n",
    "            break\n",
    "        # else\n",
    "        key = (second, third)\n",
    "        first, second = key\n",
    " \n",
    "    return ' '.join(li)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = [r for r in re.split(splitter, corpus) if r]\n",
    "words = _parse(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey.CRED SVP drifter data files have been quality controlled, calibrated, remapped and merged together to provide background data for engineers, scientific, and other commercial and industrial activities. False\n",
      "\n",
      "River and was compiled and synthesized primarily from ships and the WOCE Upper Ocean Thermal Program (WOCE UOT). False\n",
      "\n",
      "Neuroscience Information Framework enhances neuroscience research by enabling discovery and access to simple meteorological summaries for heating and cooling degree hours, pressure normals). False\n",
      "\n",
      "Because these faults are, in large part, not present in spotted owl habitat.WMSCurrent and other commercial and industrial activities. False\n",
      "\n",
      "UOT).In 1983, President Ronald Reagan signed a proclamation establishing an Exclusive Economic Zone. False\n",
      "\n",
      "Resources to provide background data for engineers, scientific, and other commercial and industrial activities. False\n",
      "\n",
      "Bay (management area 512; approximately latitude 58 degrees north and south of St. False\n",
      "\n",
      "Brookes Virtual Help on Brookes WikiThis static map image portrays legislative district is to provide scientists, managers, educational institutions (or students), and policy-makers with online access to Earth science data sets created in 1890 and established in 1974 by the USGS 1:100,000 DLG linework with the goal of the file and, indeed, state officials use the Redistricting Census 2000 TIGER/Line data provided in formats appropriate to geographic information studies and real-time data on the effects of Cold War nuclear testing.An aerial photograph with the data table includes all counties and statistically equivalent entities for purposes of examining geographic coverage for a large number of people in the 'eastern' Gulf of Mexico from 2010-08-27 to 2010-09-01 in response to requirements outlined in Lu et al. False\n",
      "\n",
      "Service delivers airports in the bottom of the Nation's ground water is more efficient. False\n",
      "\n",
      "Ages were derived from U.S. False\n",
      "\n",
      "Honolulu have received considerable perturbation over the next 20 years.This dataset consists of land derived from this cruise have EK60 data that are represented in this metadata record.The Ecotox Program is at the time of sampling, or species? False\n",
      "\n",
      "CatalogDFW collects `trip ticket? True\n",
      "\n",
      "Marine Biogeochemical Cycles of Trace Elements and their IsotopesAnnotated list of all three of the south of Vieques, Puerto Rico. False\n",
      "\n",
      "Search ProviderR.A.G.E. Search R.A.G.E. True\n",
      "\n",
      "Bias Corrected Divisional Temperature-Precipitation Drought Index. True\n",
      "\n",
      "Marine research conducted in the Well Log Observation Content Model will become more frequent with sea level (MSL) variations were analyzed along with the total population for these flights were; laser pulse rate: 25kHz, scanner rate: 26Hz, scan angle: +/- 20deg, beam divergence: narrow, altitude: 300-600m AGL, and ground water resources home page for Minnesota with links to historic droughts of Kansas, ground water, surface water, droughts, and water quality in river, aquifer and coastal topography. False\n",
      "\n",
      "Betaproteobacteria and Gammaproteobacteria) and eukarya (Alveolata, Fungi, Stramenopiles and Chloroplastida). True\n",
      "\n",
      "Mn/Ca measurements were collected using Niskin bottles. False\n",
      "\n",
      "Hispanic ethnicity for the reporting of Census Bureau is producing the Redistricting Census 2000 TIGER/Line files will not include files for each 40-acre parcel in the atmosphere and ocean basins as well as the combined effects of oil from the California Seafloor Mapping Program (NCMP). False\n",
      "\n",
      "Highway Planning Network (NTAD 2014) is a major component of ocean temperature and surface tows [Manta nets]). False\n",
      "\n",
      "Register notices04/20/2011  Call for Information02/05/2013  Call for Information07/17/2014 Proposed Sale NoticeFor more information: Florida State University. False\n",
      "\n",
      "NOS Summit-to-Sea project. True\n",
      "\n",
      "GIS (ArcMap), and both digital and hard-copy versions will be collected 24 hours a day throughout the year 2020, chemicals are produced by Science Applications, Inc., prime contractor on the TDI-Brooks research vessel, i.e. False\n",
      "\n",
      "Armenia, and other commercial and industrial activities. False\n",
      "\n",
      "UK academic community and the National Ocean Service, National Centers for Coastal Ocean Science, Center for Tsunami Research (formerly NOAA Center for Coastal Ocean Science, Center for Coastal Ocean Science, Center for Coastal Ocean Science, Center for Earth Observation from photography obtained in short-term current studies. False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's generate some abstracts!\n",
    "\n",
    "\n",
    "for i in xrange(25):\n",
    "    sentence = _generate(words)\n",
    "    print sentence, sentence in corpus\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since MPCA only assesses a small survey launch at a fixed pressure (usually between 1000 and 2000 dbar) then rise, collecting profiles of temperature on a GPS unit that is probably related to subducted slab material descending in the household and their Isotopes\"The original coverage was achieved in the Moor House region of American Indian, African American, Latino, Puerto Rican, and other federal, state, and local agencies manage resources well.Educational site for an inventory of all stocks and stock complexes that have the same depth off the northeastern shore of Puerto Rico Gap Analysis Project. Arizona, Utah, and New London. November 1977 to 16 divers. System contains daily oceanographic measurements from the Census Bureau is producing detailed geologic maps for the 2010 Census.\n",
      "\n",
      "\n",
      "Media: sections, maps, 35-mm microfilm, magnetic tapes, text and microfiche. April 2010 by the USFWS's National Wetlands Inventory project. Basin Snow-off LiDAR Survey.The European Maritime Safety Agency (EMSA) is a compilation of information sources, and history of hydrocarbon seepage and biodegradation along the coastline. A proxy for fire weather was then exported from the National Agriculture Imagery Program (NAIP); 1 meter ground sample distance (GSD) ortho imagery rectified to a listserv on the publication date of fishing gear from the record (wave amplitude) made by merging all situs points within 25 feet of undiscovered oil and 227 trillion cubic feet of a coastal survey along the NC coast. Eastern Snake Plain to improve our ability to integrate waterway data (U.S.\n",
      "\n",
      "\n",
      "A DHW measures accumulation of damaging stress, a value of these variables and the St. Basic data including KML and GIS formatted data files. Atoll. Larvae were sampled along 2 consecutively-placed, 25m transect lines. Online W rterbuch um einzelne W rter oder Phrasen vom Englischen ins Deutsche (oder umgekehrt) zu bersetzen.Find locally grown produce anywhere in the future. Map on geologic map). July 1970 to 13 November 1989.DNR land ownership and administrative purposes. University Research Archive Edinburgh Research Archive DSpace repositoryThe National Oceanic and Atmospheric Administration (NOAA) has the statutory mandate to collect hydrographic data in support of nautical chart compilation for safe navigation and to provide background data for engineers, scientific, and other federal agencies charged with reducing the risk of maritime accidents, marine pollution from ships and the National Aeronautics and Space Administration (NASA) collaborated on the collection of information on the Universal Transverse Mercator (UTM) coordinate (meters), vertical position z above the WGS84 ellipsoid (meters), radar waveform in nanosecond (10 nsec for this metadata record.Messiah College, a Christian college in PAAtlassian Confluence Search ProviderUse MODAPS Dataset Search to obtain sufficient mass and were carried out by several institutions; data were collected monthly.\n",
      "\n",
      "\n",
      "Databib RepositoriesThe National Oceanic and Atmospheric Administration (NOAA) has the statutory mandate to collect hydrographic data in support of nautical chart compilation for safe navigation and to develop coral reef ecosystems, towed-diver surveys (aka. MicroTSG and a MATE Intern (Shinobu Okano) participated in research cruises each year for large geographic areas, those with 65,000 persons or more. March 1983, President Ronald Reagan signed a proclamation establishing an Exclusive Economic Zone, and 200 nautical miles from the original terrain data. Spectroradiometer (RSS) implements the data by 3x3 degree tiles in GeoTIFF format. Surveyor. The purpose of census tracts occasionally are split due to the NHD, and the Navajo Nation community of practice that recognizes the interconnections between the same vintage as those appearing in the eastern edge, and regularly spaced intervals.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# those are fantastically not good.\n",
    "# so let's add a sentence count to get\n",
    "# something like a 3-7 sentence abstract, also random\n",
    "\n",
    "def generate_abstract():\n",
    "    sentence_length = randint(3, 7)\n",
    "    abstract = []\n",
    "    for i in xrange(sentence_length):\n",
    "        abstract.append( _generate(words))\n",
    "    return ' '.join(abstract)\n",
    "\n",
    "for i in xrange(4):\n",
    "    print generate_abstract()\n",
    "    print\n",
    "    print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's try some haikus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "haiku = (5, 7, 5)\n",
    "syllable_dict = \n",
    "\n",
    "def generate_line(chain, syllable_count, current):\n",
    "    if len(chain[current]) == 1:\n",
    "        out = chain[current][0]\n",
    "    else:\n",
    "        out = chain[current][randint(0, len(chain[current]) -1)]\n",
    "    \n",
    "    current_syllables = 0\n",
    "    \n",
    "    # TODO: deal with the corpus to dict situation\n",
    "    \n",
    "    \n",
    "    if out in syllable_dict\n",
    "\n",
    "# we need a new corpus structure as markov chain\n",
    "terms = [r for r in re.split(splitter, corpus) if r]\n",
    "markov_chain = {}\n",
    "\n",
    "for i in xrange(len(terms)-1):\n",
    "    if terms[i] in markov_chain:\n",
    "        markov_chain[terms[i]].append(terms[i+1])\n",
    "    else:\n",
    "        markov_chain[terms[i]] = [terms[i+1]]\n",
    "        \n",
    "\n",
    "# start the poem\n",
    "poem = [terms[randint(0, len(terms))]]\n",
    "\n",
    "for s in haiku:\n",
    "    line = None\n",
    "    while line is None:\n",
    "        line = generate_line(markov_chain, s, poem[-1].split()[-1])\n",
    "    poem.append(line)\n",
    "\n",
    "    \n",
    "print poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "fundamental problem here - cmudict contains things like : ognibene but probably not a chunk of our scientific text. and syllable identification is not easy. i am going with some combo of matching terms existing in cmudict and then a python version of the tex hyphenator for the rest. fingers crossed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
